{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de45c07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (646, 33)\n",
      "Test: (277, 32)\n",
      "    id  age_first_funding_year  age_last_funding_year  \\\n",
      "0  719                   10.42                  13.09   \n",
      "1  429                    3.79                   3.79   \n",
      "2  178                    0.71                   2.28   \n",
      "3  197                    3.00                   5.00   \n",
      "4  444                    0.66                   5.88   \n",
      "\n",
      "   age_first_milestone_year  age_last_milestone_year  relationships  \\\n",
      "0                      8.98                    12.72              4   \n",
      "1                       NaN                      NaN             21   \n",
      "2                      1.95                     2.28              5   \n",
      "3                      9.62                    10.39             16   \n",
      "4                      6.21                     8.61             29   \n",
      "\n",
      "   funding_rounds  funding_total_usd  milestones  is_CA  ...  is_consulting  \\\n",
      "0               3            4087500           3      1  ...              0   \n",
      "1               1           45000000           0      0  ...              0   \n",
      "2               2            5200000           2      1  ...              0   \n",
      "3               2           14500000           2      0  ...              0   \n",
      "4               5           70000000           4      1  ...              0   \n",
      "\n",
      "   is_othercategory  has_VC  has_angel has_roundA  has_roundB  has_roundC  \\\n",
      "0                 0       1          1          0           0           0   \n",
      "1                 0       0          0          0           1           0   \n",
      "2                 1       1          0          1           0           0   \n",
      "3                 0       0          1          0           1           0   \n",
      "4                 0       0          0          1           1           1   \n",
      "\n",
      "   has_roundD  avg_participants  labels  \n",
      "0           0               1.0       0  \n",
      "1           0               1.0       1  \n",
      "2           0               1.0       0  \n",
      "3           0               2.0       1  \n",
      "4           1               2.8       1  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "                id  age_first_funding_year  age_last_funding_year  \\\n",
      "count   646.000000              611.000000             637.000000   \n",
      "unique         NaN                     NaN                    NaN   \n",
      "top            NaN                     NaN                    NaN   \n",
      "freq           NaN                     NaN                    NaN   \n",
      "mean    461.577399                2.341718               4.037724   \n",
      "std     264.859464                2.468275               2.950923   \n",
      "min       1.000000                0.000000               0.000000   \n",
      "25%     233.250000                0.680000               1.870000   \n",
      "50%     459.500000                1.650000               3.610000   \n",
      "75%     692.500000                3.600000               5.590000   \n",
      "max     923.000000               21.900000              21.900000   \n",
      "\n",
      "        age_first_milestone_year  age_last_milestone_year  relationships  \\\n",
      "count                 508.000000               535.000000     646.000000   \n",
      "unique                       NaN                      NaN            NaN   \n",
      "top                          NaN                      NaN            NaN   \n",
      "freq                         NaN                      NaN            NaN   \n",
      "mean                    3.352657                 4.944729       7.948916   \n",
      "std                     2.866952                 3.213319       7.397602   \n",
      "min                     0.000000                 0.000000       0.000000   \n",
      "25%                     1.185000                 2.540000       3.000000   \n",
      "50%                     2.785000                 4.620000       6.000000   \n",
      "75%                     4.935000                 6.880000      10.000000   \n",
      "max                    24.680000                24.680000      63.000000   \n",
      "\n",
      "        funding_rounds  funding_total_usd  milestones       is_CA  ...  \\\n",
      "count       646.000000       6.460000e+02  646.000000  646.000000  ...   \n",
      "unique             NaN                NaN         NaN         NaN  ...   \n",
      "top                NaN                NaN         NaN         NaN  ...   \n",
      "freq               NaN                NaN         NaN         NaN  ...   \n",
      "mean          2.351393       2.949633e+07    1.913313    0.546440  ...   \n",
      "std           1.357856       2.261999e+08    1.337095    0.498224  ...   \n",
      "min           1.000000       1.100000e+04    0.000000    0.000000  ...   \n",
      "25%           1.000000       3.000000e+06    1.000000    0.000000  ...   \n",
      "50%           2.000000       1.020000e+07    2.000000    1.000000  ...   \n",
      "75%           3.000000       2.587500e+07    3.000000    1.000000  ...   \n",
      "max           8.000000       5.700000e+09    6.000000    1.000000  ...   \n",
      "\n",
      "        is_consulting  is_othercategory      has_VC   has_angel  has_roundA  \\\n",
      "count      646.000000        646.000000  646.000000  646.000000  646.000000   \n",
      "unique            NaN               NaN         NaN         NaN         NaN   \n",
      "top               NaN               NaN         NaN         NaN         NaN   \n",
      "freq              NaN               NaN         NaN         NaN         NaN   \n",
      "mean         0.003096          0.304954    0.329721    0.260062    0.515480   \n",
      "std          0.055598          0.460745    0.470476    0.439008    0.500148   \n",
      "min          0.000000          0.000000    0.000000    0.000000    0.000000   \n",
      "25%          0.000000          0.000000    0.000000    0.000000    0.000000   \n",
      "50%          0.000000          0.000000    0.000000    0.000000    1.000000   \n",
      "75%          0.000000          1.000000    1.000000    1.000000    1.000000   \n",
      "max          1.000000          1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "        has_roundB  has_roundC  has_roundD  avg_participants      labels  \n",
      "count   646.000000  646.000000  646.000000        646.000000  646.000000  \n",
      "unique         NaN         NaN         NaN               NaN         NaN  \n",
      "top            NaN         NaN         NaN               NaN         NaN  \n",
      "freq           NaN         NaN         NaN               NaN         NaN  \n",
      "mean      0.419505    0.235294    0.091331          2.848655    0.647059  \n",
      "std       0.493860    0.424511    0.288303          1.894050    0.478255  \n",
      "min       0.000000    0.000000    0.000000          1.000000    0.000000  \n",
      "25%       0.000000    0.000000    0.000000          1.500000    0.000000  \n",
      "50%       0.000000    0.000000    0.000000          2.333300    1.000000  \n",
      "75%       1.000000    0.000000    0.000000          4.000000    1.000000  \n",
      "max       1.000000    1.000000    1.000000         16.000000    1.000000  \n",
      "\n",
      "[11 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar datasets\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Ver dimensões\n",
    "print(\"Train:\", train.shape)\n",
    "print(\"Test:\", test.shape)\n",
    "\n",
    "# Olhar primeiras linhas\n",
    "print(train.head())\n",
    "\n",
    "# Ver estatísticas gerais\n",
    "print(train.describe(include=\"all\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "362ccb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INFORMAÇÕES GERAIS ===\n",
      "Train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 646 entries, 0 to 645\n",
      "Data columns (total 33 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   id                        646 non-null    int64  \n",
      " 1   age_first_funding_year    611 non-null    float64\n",
      " 2   age_last_funding_year     637 non-null    float64\n",
      " 3   age_first_milestone_year  508 non-null    float64\n",
      " 4   age_last_milestone_year   535 non-null    float64\n",
      " 5   relationships             646 non-null    int64  \n",
      " 6   funding_rounds            646 non-null    int64  \n",
      " 7   funding_total_usd         646 non-null    int64  \n",
      " 8   milestones                646 non-null    int64  \n",
      " 9   is_CA                     646 non-null    int64  \n",
      " 10  is_NY                     646 non-null    int64  \n",
      " 11  is_MA                     646 non-null    int64  \n",
      " 12  is_TX                     646 non-null    int64  \n",
      " 13  is_otherstate             646 non-null    int64  \n",
      " 14  category_code             646 non-null    object \n",
      " 15  is_software               646 non-null    int64  \n",
      " 16  is_web                    646 non-null    int64  \n",
      " 17  is_mobile                 646 non-null    int64  \n",
      " 18  is_enterprise             646 non-null    int64  \n",
      " 19  is_advertising            646 non-null    int64  \n",
      " 20  is_gamesvideo             646 non-null    int64  \n",
      " 21  is_ecommerce              646 non-null    int64  \n",
      " 22  is_biotech                646 non-null    int64  \n",
      " 23  is_consulting             646 non-null    int64  \n",
      " 24  is_othercategory          646 non-null    int64  \n",
      " 25  has_VC                    646 non-null    int64  \n",
      " 26  has_angel                 646 non-null    int64  \n",
      " 27  has_roundA                646 non-null    int64  \n",
      " 28  has_roundB                646 non-null    int64  \n",
      " 29  has_roundC                646 non-null    int64  \n",
      " 30  has_roundD                646 non-null    int64  \n",
      " 31  avg_participants          646 non-null    float64\n",
      " 32  labels                    646 non-null    int64  \n",
      "dtypes: float64(5), int64(27), object(1)\n",
      "memory usage: 166.7+ KB\n",
      "None\n",
      "\n",
      "Test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 277 entries, 0 to 276\n",
      "Data columns (total 32 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   id                        277 non-null    int64  \n",
      " 1   age_first_funding_year    266 non-null    float64\n",
      " 2   age_last_funding_year     273 non-null    float64\n",
      " 3   age_first_milestone_year  217 non-null    float64\n",
      " 4   age_last_milestone_year   224 non-null    float64\n",
      " 5   relationships             277 non-null    int64  \n",
      " 6   funding_rounds            277 non-null    int64  \n",
      " 7   funding_total_usd         277 non-null    int64  \n",
      " 8   milestones                277 non-null    int64  \n",
      " 9   is_CA                     277 non-null    int64  \n",
      " 10  is_NY                     277 non-null    int64  \n",
      " 11  is_MA                     277 non-null    int64  \n",
      " 12  is_TX                     277 non-null    int64  \n",
      " 13  is_otherstate             277 non-null    int64  \n",
      " 14  category_code             277 non-null    object \n",
      " 15  is_software               277 non-null    int64  \n",
      " 16  is_web                    277 non-null    int64  \n",
      " 17  is_mobile                 277 non-null    int64  \n",
      " 18  is_enterprise             277 non-null    int64  \n",
      " 19  is_advertising            277 non-null    int64  \n",
      " 20  is_gamesvideo             277 non-null    int64  \n",
      " 21  is_ecommerce              277 non-null    int64  \n",
      " 22  is_biotech                277 non-null    int64  \n",
      " 23  is_consulting             277 non-null    int64  \n",
      " 24  is_othercategory          277 non-null    int64  \n",
      " 25  has_VC                    277 non-null    int64  \n",
      " 26  has_angel                 277 non-null    int64  \n",
      " 27  has_roundA                277 non-null    int64  \n",
      " 28  has_roundB                277 non-null    int64  \n",
      " 29  has_roundC                277 non-null    int64  \n",
      " 30  has_roundD                277 non-null    int64  \n",
      " 31  avg_participants          277 non-null    float64\n",
      "dtypes: float64(5), int64(26), object(1)\n",
      "memory usage: 69.4+ KB\n",
      "None\n",
      "\n",
      "=== VALORES AUSENTES ===\n",
      "Train - valores nulos:\n",
      "id                            0\n",
      "age_first_funding_year       35\n",
      "age_last_funding_year         9\n",
      "age_first_milestone_year    138\n",
      "age_last_milestone_year     111\n",
      "relationships                 0\n",
      "funding_rounds                0\n",
      "funding_total_usd             0\n",
      "milestones                    0\n",
      "is_CA                         0\n",
      "is_NY                         0\n",
      "is_MA                         0\n",
      "is_TX                         0\n",
      "is_otherstate                 0\n",
      "category_code                 0\n",
      "is_software                   0\n",
      "is_web                        0\n",
      "is_mobile                     0\n",
      "is_enterprise                 0\n",
      "is_advertising                0\n",
      "is_gamesvideo                 0\n",
      "is_ecommerce                  0\n",
      "is_biotech                    0\n",
      "is_consulting                 0\n",
      "is_othercategory              0\n",
      "has_VC                        0\n",
      "has_angel                     0\n",
      "has_roundA                    0\n",
      "has_roundB                    0\n",
      "has_roundC                    0\n",
      "has_roundD                    0\n",
      "avg_participants              0\n",
      "labels                        0\n",
      "dtype: int64\n",
      "\n",
      "Test - valores nulos:\n",
      "id                           0\n",
      "age_first_funding_year      11\n",
      "age_last_funding_year        4\n",
      "age_first_milestone_year    60\n",
      "age_last_milestone_year     53\n",
      "relationships                0\n",
      "funding_rounds               0\n",
      "funding_total_usd            0\n",
      "milestones                   0\n",
      "is_CA                        0\n",
      "is_NY                        0\n",
      "is_MA                        0\n",
      "is_TX                        0\n",
      "is_otherstate                0\n",
      "category_code                0\n",
      "is_software                  0\n",
      "is_web                       0\n",
      "is_mobile                    0\n",
      "is_enterprise                0\n",
      "is_advertising               0\n",
      "is_gamesvideo                0\n",
      "is_ecommerce                 0\n",
      "is_biotech                   0\n",
      "is_consulting                0\n",
      "is_othercategory             0\n",
      "has_VC                       0\n",
      "has_angel                    0\n",
      "has_roundA                   0\n",
      "has_roundB                   0\n",
      "has_roundC                   0\n",
      "has_roundD                   0\n",
      "avg_participants             0\n",
      "dtype: int64\n",
      "\n",
      "=== DUPLICATAS ===\n",
      "Train duplicatas: 0\n",
      "Test duplicatas: 0\n"
     ]
    }
   ],
   "source": [
    "# ANÁLISE EXPLORATÓRIA INICIAL\n",
    "# Verificar informações gerais dos dados\n",
    "print(\"=== INFORMAÇÕES GERAIS ===\")\n",
    "print(\"Train info:\")\n",
    "print(train.info())\n",
    "print(\"\\nTest info:\")\n",
    "print(test.info())\n",
    "\n",
    "# Verificar valores ausentes\n",
    "print(\"\\n=== VALORES AUSENTES ===\")\n",
    "print(\"Train - valores nulos:\")\n",
    "print(train.isnull().sum())\n",
    "print(\"\\nTest - valores nulos:\")\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# Verificar duplicatas\n",
    "print(\"\\n=== DUPLICATAS ===\")\n",
    "print(f\"Train duplicatas: {train.duplicated().sum()}\")\n",
    "print(f\"Test duplicatas: {test.duplicated().sum()}\")\n",
    "\n",
    "# Analisar a variável target (se existir)\n",
    "if 'target' in train.columns or 'success' in train.columns:\n",
    "    target_col = 'target' if 'target' in train.columns else 'success'\n",
    "    print(f\"\\n=== DISTRIBUIÇÃO DO TARGET ({target_col}) ===\")\n",
    "    print(train[target_col].value_counts())\n",
    "    print(f\"Proporção: {train[target_col].value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4252d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TIPOS DE COLUNAS ===\n",
      "Colunas numéricas: ['id', 'age_first_funding_year', 'age_last_funding_year', 'age_first_milestone_year', 'age_last_milestone_year', 'relationships', 'funding_rounds', 'funding_total_usd', 'milestones', 'is_CA', 'is_NY', 'is_MA', 'is_TX', 'is_otherstate', 'is_software', 'is_web', 'is_mobile', 'is_enterprise', 'is_advertising', 'is_gamesvideo', 'is_ecommerce', 'is_biotech', 'is_consulting', 'is_othercategory', 'has_VC', 'has_angel', 'has_roundA', 'has_roundB', 'has_roundC', 'has_roundD', 'avg_participants', 'labels']\n",
      "Colunas categóricas: ['category_code']\n"
     ]
    }
   ],
   "source": [
    "#LIMPEZA E PROCESSAMENTO DOS DADOS\n",
    "\n",
    "# 1. Remover duplicatas (se houver)\n",
    "train = train.drop_duplicates()\n",
    "test = test.drop_duplicates()\n",
    "\n",
    "# 2. Identificar tipos de colunas\n",
    "print(\"\\n=== TIPOS DE COLUNAS ===\")\n",
    "numerical_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Colunas numéricas: {numerical_cols}\")\n",
    "print(f\"Colunas categóricas: {categorical_cols}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9107c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tratamento de valores ausentes\n",
    "def tratar_valores_ausentes(df, is_train=True):\n",
    "    \"\"\"Função para tratar valores ausentes\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Para colunas numéricas: preencher com mediana\n",
    "    for col in numerical_cols:\n",
    "        if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "    \n",
    "    # Para colunas categóricas: preencher com moda ou 'Unknown'\n",
    "    for col in categorical_cols:\n",
    "        if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
    "            # Se for poucas categorias, usar moda; senão usar 'Unknown'\n",
    "            if df_clean[col].nunique() < 20:\n",
    "                df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "            else:\n",
    "                df_clean[col].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Aplicar tratamento\n",
    "train_clean = tratar_valores_ausentes(train, is_train=True)\n",
    "test_clean = tratar_valores_ausentes(test, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "facfcc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OUTLIERS DETECTADOS ===\n",
      "id: 0 outliers (0.00%)\n",
      "age_first_funding_year: 20 outliers (3.10%)\n",
      "age_last_funding_year: 11 outliers (1.70%)\n",
      "age_first_milestone_year: 41 outliers (6.35%)\n",
      "age_last_milestone_year: 22 outliers (3.41%)\n",
      "relationships: 47 outliers (7.28%)\n",
      "funding_rounds: 10 outliers (1.55%)\n",
      "funding_total_usd: 50 outliers (7.74%)\n",
      "milestones: 0 outliers (0.00%)\n",
      "is_CA: 0 outliers (0.00%)\n",
      "is_NY: 71 outliers (10.99%)\n",
      "is_MA: 61 outliers (9.44%)\n",
      "is_TX: 24 outliers (3.72%)\n",
      "is_otherstate: 136 outliers (21.05%)\n",
      "is_software: 105 outliers (16.25%)\n",
      "is_web: 97 outliers (15.02%)\n",
      "is_mobile: 65 outliers (10.06%)\n",
      "is_enterprise: 53 outliers (8.20%)\n",
      "is_advertising: 45 outliers (6.97%)\n",
      "is_gamesvideo: 37 outliers (5.73%)\n",
      "is_ecommerce: 20 outliers (3.10%)\n",
      "is_biotech: 25 outliers (3.87%)\n",
      "is_consulting: 2 outliers (0.31%)\n",
      "is_othercategory: 0 outliers (0.00%)\n",
      "has_VC: 0 outliers (0.00%)\n",
      "has_angel: 0 outliers (0.00%)\n",
      "has_roundA: 0 outliers (0.00%)\n",
      "has_roundB: 0 outliers (0.00%)\n",
      "has_roundC: 152 outliers (23.53%)\n",
      "has_roundD: 59 outliers (9.13%)\n",
      "avg_participants: 19 outliers (2.94%)\n",
      "labels: 0 outliers (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# 4. Análise de outliers (para colunas numéricas)\n",
    "def detectar_outliers(df, cols):\n",
    "    \"\"\"Detecta outliers usando IQR\"\"\"\n",
    "    outliers_info = {}\n",
    "    \n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outliers_info[col] = {\n",
    "                'count': len(outliers),\n",
    "                'percentage': len(outliers) / len(df) * 100,\n",
    "                'bounds': (lower_bound, upper_bound)\n",
    "            }\n",
    "    \n",
    "    return outliers_info\n",
    "\n",
    "# Detectar outliers\n",
    "outliers_train = detectar_outliers(train_clean, numerical_cols)\n",
    "print(\"\\n=== OUTLIERS DETECTADOS ===\")\n",
    "for col, info in outliers_train.items():\n",
    "    print(f\"{col}: {info['count']} outliers ({info['percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54665139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando coluna: category_code\n",
      "  → Label Encoding (34 categorias)\n",
      "    ⚠️  Categorias novas no test: {'hospitality'}\n",
      "    📝 Adicionadas 5 linhas 'Unknown' ao train\n"
     ]
    }
   ],
   "source": [
    "# 5. Codificação de variáveis categóricas - VERSÃO CORRIGIDA\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def processar_categoricas(train_df, test_df, categorical_columns):\n",
    "    \"\"\"Processa variáveis categóricas - versão que lida com categorias novas no test\"\"\"\n",
    "    train_processed = train_df.copy()\n",
    "    test_processed = test_df.copy()\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in train_processed.columns:\n",
    "            print(f\"\\nProcessando coluna: {col}\")\n",
    "            \n",
    "            # Se poucas categorias, usar One-Hot Encoding\n",
    "            if train_processed[col].nunique() <= 10:\n",
    "                print(f\"  → One-Hot Encoding ({train_processed[col].nunique()} categorias)\")\n",
    "                \n",
    "                # Identificar todas as categorias únicas entre train e test\n",
    "                all_categories = set(train_processed[col].unique()) | set(test_processed[col].unique())\n",
    "                \n",
    "                # One-Hot Encoding\n",
    "                train_dummies = pd.get_dummies(train_processed[col], prefix=col)\n",
    "                test_dummies = pd.get_dummies(test_processed[col], prefix=col)\n",
    "                \n",
    "                # Garantir mesmas colunas em train e test\n",
    "                for category in all_categories:\n",
    "                    dummy_col = f\"{col}_{category}\"\n",
    "                    if dummy_col not in train_dummies.columns:\n",
    "                        train_dummies[dummy_col] = 0\n",
    "                    if dummy_col not in test_dummies.columns:\n",
    "                        test_dummies[dummy_col] = 0\n",
    "                \n",
    "                # Ordenar colunas para garantir mesma ordem\n",
    "                dummy_columns = sorted(train_dummies.columns)\n",
    "                train_dummies = train_dummies[dummy_columns]\n",
    "                test_dummies = test_dummies[dummy_columns]\n",
    "                \n",
    "                # Remover coluna original e adicionar dummies\n",
    "                train_processed = train_processed.drop(col, axis=1)\n",
    "                test_processed = test_processed.drop(col, axis=1)\n",
    "                train_processed = pd.concat([train_processed, train_dummies], axis=1)\n",
    "                test_processed = pd.concat([test_processed, test_dummies], axis=1)\n",
    "            \n",
    "            else:\n",
    "                print(f\"  → Label Encoding ({train_processed[col].nunique()} categorias)\")\n",
    "                \n",
    "                # CORREÇÃO ROBUSTA: Lidar com categorias novas no test\n",
    "                # 1. Converter para string primeiro\n",
    "                train_processed[col] = train_processed[col].astype(str)\n",
    "                test_processed[col] = test_processed[col].astype(str)\n",
    "                \n",
    "                # 2. Obter todas as categorias únicas\n",
    "                train_categories = set(train_processed[col].unique())\n",
    "                test_categories = set(test_processed[col].unique())\n",
    "                \n",
    "                # 3. Categorias que estão no test mas não no train\n",
    "                new_categories = test_categories - train_categories\n",
    "                if new_categories:\n",
    "                    print(f\"    ⚠️  Categorias novas no test: {new_categories}\")\n",
    "                    # Mapear categorias novas para 'Unknown'\n",
    "                    for new_cat in new_categories:\n",
    "                        test_processed.loc[test_processed[col] == new_cat, col] = 'Unknown'\n",
    "                    \n",
    "                    # ADICIONANDO 'Unknown' ao train se necessário\n",
    "                    if 'Unknown' not in train_categories:\n",
    "                        # Adicionar 'Unknown' a algumas linhas do train\n",
    "                        n_unknown = min(5, len(train_processed) // 100)  # 1% ou 5 linhas, o que for menor\n",
    "                        unknown_indices = train_processed.sample(n=n_unknown).index\n",
    "                        train_processed.loc[unknown_indices, col] = 'Unknown'\n",
    "                        print(f\"    📝 Adicionadas {n_unknown} linhas 'Unknown' ao train\")\n",
    "                \n",
    "                # 4. Aplicar Label Encoding com tratamento de erro\n",
    "                try:\n",
    "                    le = LabelEncoder()\n",
    "                    train_processed[col] = le.fit_transform(train_processed[col])\n",
    "                    test_processed[col] = le.transform(test_processed[col])\n",
    "                except ValueError as e:\n",
    "                    print(f\"    🚨 Erro no Label Encoding para {col}: {e}\")\n",
    "                    # Fallback: usar One-Hot mesmo com muitas categorias\n",
    "                    print(f\"    🔄 Usando One-Hot como fallback\")\n",
    "                    train_dummies = pd.get_dummies(train_processed[col], prefix=col)\n",
    "                    test_dummies = pd.get_dummies(test_processed[col], prefix=col)\n",
    "                    \n",
    "                    # Garantir mesmas colunas\n",
    "                    all_columns = set(train_dummies.columns) | set(test_dummies.columns)\n",
    "                    for dummy_col in all_columns:\n",
    "                        if dummy_col not in train_dummies.columns:\n",
    "                            train_dummies[dummy_col] = 0\n",
    "                        if dummy_col not in test_dummies.columns:\n",
    "                            test_dummies[dummy_col] = 0\n",
    "                    \n",
    "                    # Substituir coluna\n",
    "                    train_processed = train_processed.drop(col, axis=1)\n",
    "                    test_processed = test_processed.drop(col, axis=1)\n",
    "                    train_processed = pd.concat([train_processed, train_dummies], axis=1)\n",
    "                    test_processed = pd.concat([test_processed, test_dummies], axis=1)\n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "# Aplicar processamento categórico com correção\n",
    "train_processed, test_processed = processar_categoricas(train_clean, test_clean, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d6c6af23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas numéricas para normalizar: ['id', 'age_first_funding_year', 'age_last_funding_year', 'age_first_milestone_year', 'age_last_milestone_year', 'relationships', 'funding_rounds', 'funding_total_usd', 'milestones', 'is_CA', 'is_NY', 'is_MA', 'is_TX', 'is_otherstate', 'is_software', 'is_web', 'is_mobile', 'is_enterprise', 'is_advertising', 'is_gamesvideo', 'is_ecommerce', 'is_biotech', 'is_consulting', 'is_othercategory', 'has_VC', 'has_angel', 'has_roundA', 'has_roundB', 'has_roundC', 'has_roundD', 'avg_participants']\n",
      "  ✅ id normalizado\n",
      "  ✅ age_first_funding_year normalizado\n",
      "  ✅ age_last_funding_year normalizado\n",
      "  ✅ age_first_milestone_year normalizado\n",
      "  ✅ age_last_milestone_year normalizado\n",
      "  ✅ relationships normalizado\n",
      "  ✅ funding_rounds normalizado\n",
      "  ✅ funding_total_usd normalizado\n",
      "  ✅ milestones normalizado\n",
      "  ✅ is_CA normalizado\n",
      "  ✅ is_NY normalizado\n",
      "  ✅ is_MA normalizado\n",
      "  ✅ is_TX normalizado\n",
      "  ✅ is_otherstate normalizado\n",
      "  ✅ is_software normalizado\n",
      "  ✅ is_web normalizado\n",
      "  ✅ is_mobile normalizado\n",
      "  ✅ is_enterprise normalizado\n",
      "  ✅ is_advertising normalizado\n",
      "  ✅ is_gamesvideo normalizado\n",
      "  ✅ is_ecommerce normalizado\n",
      "  ✅ is_biotech normalizado\n",
      "  ✅ is_consulting normalizado\n",
      "  ✅ is_othercategory normalizado\n",
      "  ✅ has_VC normalizado\n",
      "  ✅ has_angel normalizado\n",
      "  ✅ has_roundA normalizado\n",
      "  ✅ has_roundB normalizado\n",
      "  ✅ has_roundC normalizado\n",
      "  ✅ has_roundD normalizado\n",
      "  ✅ avg_participants normalizado\n"
     ]
    }
   ],
   "source": [
    "# 6. Normalização/Padronização de variáveis numéricas\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def normalizar_dados(train_df, test_df, numerical_columns, method='standard'):\n",
    "    \"\"\"Normaliza dados numéricos\"\"\"\n",
    "    train_norm = train_df.copy()\n",
    "    test_norm = test_df.copy()\n",
    "    \n",
    "    # Verificar quais colunas numéricas ainda existem após processamento\n",
    "    existing_num_cols = [col for col in numerical_columns if col in train_norm.columns and col in test_norm.columns]\n",
    "    \n",
    "    print(f\"Colunas numéricas para normalizar: {existing_num_cols}\")\n",
    "    \n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    for col in existing_num_cols:\n",
    "        if col in train_norm.columns and col in test_norm.columns:\n",
    "            # Normalizar cada coluna individualmente\n",
    "            scaler_col = StandardScaler() if method == 'standard' else MinMaxScaler()\n",
    "            train_norm[col] = scaler_col.fit_transform(train_norm[[col]]).ravel()\n",
    "            test_norm[col] = scaler_col.transform(test_norm[[col]]).ravel()\n",
    "            print(f\"  ✅ {col} normalizado\")\n",
    "    \n",
    "    return train_norm, test_norm\n",
    "\n",
    "# Aplicar normalização usando lista atualizada\n",
    "train_final, test_final = normalizar_dados(train_processed, test_processed, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6208050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICAÇÃO APÓS PROCESSAMENTO CATEGÓRICO ===\n",
      "Train shape: (646, 33)\n",
      "Test shape: (277, 32)\n",
      "Colunas train: ['id', 'age_first_funding_year', 'age_last_funding_year', 'age_first_milestone_year', 'age_last_milestone_year', 'relationships', 'funding_rounds', 'funding_total_usd', 'milestones', 'is_CA', 'is_NY', 'is_MA', 'is_TX', 'is_otherstate', 'category_code', 'is_software', 'is_web', 'is_mobile', 'is_enterprise', 'is_advertising', 'is_gamesvideo', 'is_ecommerce', 'is_biotech', 'is_consulting', 'is_othercategory', 'has_VC', 'has_angel', 'has_roundA', 'has_roundB', 'has_roundC', 'has_roundD', 'avg_participants', 'labels']\n",
      "Colunas test: ['id', 'age_first_funding_year', 'age_last_funding_year', 'age_first_milestone_year', 'age_last_milestone_year', 'relationships', 'funding_rounds', 'funding_total_usd', 'milestones', 'is_CA', 'is_NY', 'is_MA', 'is_TX', 'is_otherstate', 'category_code', 'is_software', 'is_web', 'is_mobile', 'is_enterprise', 'is_advertising', 'is_gamesvideo', 'is_ecommerce', 'is_biotech', 'is_consulting', 'is_othercategory', 'has_VC', 'has_angel', 'has_roundA', 'has_roundB', 'has_roundC', 'has_roundD', 'avg_participants']\n"
     ]
    }
   ],
   "source": [
    "# Verificar estado dos dados após processamento categórico\n",
    "print(\"=== VERIFICAÇÃO APÓS PROCESSAMENTO CATEGÓRICO ===\")\n",
    "print(f\"Train shape: {train_processed.shape}\")\n",
    "print(f\"Test shape: {test_processed.shape}\")\n",
    "print(f\"Colunas train: {list(train_processed.columns)}\")\n",
    "print(f\"Colunas test: {list(test_processed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fdccb5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICAÇÃO FINAL DOS DADOS PROCESSADOS ===\n",
      "📏 Train shape: (646, 33)\n",
      "📏 Test shape: (277, 32)\n",
      "🎯 Target identificado: labels\n",
      "🔄 Colunas em comum: 32\n",
      "❌ Só no train: set()\n",
      "❌ Só no test: set()\n",
      "\n",
      "🔍 Valores nulos train: 0\n",
      "🔍 Valores nulos test: 0\n",
      "\n",
      "📊 Tipos de dados train:\n",
      "float64    31\n",
      "int32       1\n",
      "int64       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ DADOS PRONTOS PARA MODELAGEM:\n",
      "   X_train: (646, 32)\n",
      "   y_train: (646,)\n",
      "   X_test: (277, 32)\n",
      "\n",
      "🎯 Distribuição do target:\n",
      "labels\n",
      "1    418\n",
      "0    228\n",
      "Name: count, dtype: int64\n",
      "   Proporção: labels\n",
      "1    0.647059\n",
      "0    0.352941\n",
      "Name: proportion, dtype: float64\n",
      "✅ Classes razoavelmente balanceadas (ratio: 0.545)\n"
     ]
    }
   ],
   "source": [
    "# VERIFICAÇÃO FINAL COMPLETA\n",
    "print(\"=== VERIFICAÇÃO FINAL DOS DADOS PROCESSADOS ===\")\n",
    "\n",
    "# 1. Verificar shapes\n",
    "print(f\"📏 Train shape: {train_final.shape}\")\n",
    "print(f\"📏 Test shape: {test_final.shape}\")\n",
    "\n",
    "# 2. Verificar se as colunas são iguais (exceto target)\n",
    "train_cols = set(train_final.columns)\n",
    "test_cols = set(test_final.columns)\n",
    "\n",
    "# Identificar possível coluna target\n",
    "possible_targets = ['labels', 'target', 'success', 'y']\n",
    "target_col = None\n",
    "for col in possible_targets:\n",
    "    if col in train_cols and col not in test_cols:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col:\n",
    "    print(f\"🎯 Target identificado: {target_col}\")\n",
    "    train_cols.remove(target_col)\n",
    "else:\n",
    "    print(\"⚠️  Target não identificado claramente\")\n",
    "\n",
    "# 3. Verificar compatibilidade das colunas\n",
    "print(f\"🔄 Colunas em comum: {len(train_cols & test_cols)}\")\n",
    "print(f\"❌ Só no train: {train_cols - test_cols}\")\n",
    "print(f\"❌ Só no test: {test_cols - train_cols}\")\n",
    "\n",
    "# 4. Verificar valores ausentes\n",
    "print(f\"\\n🔍 Valores nulos train: {train_final.isnull().sum().sum()}\")\n",
    "print(f\"🔍 Valores nulos test: {test_final.isnull().sum().sum()}\")\n",
    "\n",
    "# 5. Verificar tipos de dados\n",
    "print(f\"\\n📊 Tipos de dados train:\")\n",
    "print(train_final.dtypes.value_counts())\n",
    "\n",
    "# 6. Preparar dados para modelagem\n",
    "if target_col:\n",
    "    # Separar features e target\n",
    "    X_train = train_final.drop(target_col, axis=1)\n",
    "    y_train = train_final[target_col]\n",
    "    X_test = test_final.copy()\n",
    "    \n",
    "    print(f\"\\n✅ DADOS PRONTOS PARA MODELAGEM:\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    print(f\"   X_test: {X_test.shape}\")\n",
    "    \n",
    "    # Verificar distribuição do target\n",
    "    print(f\"\\n🎯 Distribuição do target:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(f\"   Proporção: {y_train.value_counts(normalize=True)}\")\n",
    "    \n",
    "    # Verificar se há desbalanceamento\n",
    "    if len(y_train.value_counts()) == 2:\n",
    "        minority_class = y_train.value_counts().min()\n",
    "        majority_class = y_train.value_counts().max()\n",
    "        ratio = minority_class / majority_class\n",
    "        \n",
    "        if ratio < 0.3:\n",
    "            print(f\"⚠️  ATENÇÃO: Classes desbalanceadas (ratio: {ratio:.3f})\")\n",
    "            print(\"   Considere usar técnicas de balanceamento ou métricas adequadas\")\n",
    "        else:\n",
    "            print(f\"✅ Classes razoavelmente balanceadas (ratio: {ratio:.3f})\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ PROBLEMA: Target não identificado - verifique os dados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7479d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Divisão dos dados:\n",
      "   Treino: (516, 32)\n",
      "   Validação: (130, 32)\n",
      "\n",
      "🔧 Treinando Random Forest...\n",
      "   CV AUC: 0.807 ± 0.056\n",
      "   Val AUC: 0.803\n",
      "\n",
      "🔧 Treinando Gradient Boosting...\n",
      "   CV AUC: 0.776 ± 0.060\n",
      "   Val AUC: 0.824\n",
      "\n",
      "🔧 Treinando Logistic Regression...\n",
      "   CV AUC: 0.763 ± 0.061\n",
      "   Val AUC: 0.770\n",
      "\n",
      "🏆 Melhor modelo: Gradient Boosting\n",
      "   AUC: 0.824\n"
     ]
    }
   ],
   "source": [
    "# MODELAGEM - MÚLTIPLOS ALGORITMOS\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Dividir dados para validação\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"📊 Divisão dos dados:\")\n",
    "print(f\"   Treino: {X_train_split.shape}\")\n",
    "print(f\"   Validação: {X_val.shape}\")\n",
    "\n",
    "# 2. Testar múltiplos modelos\n",
    "modelos = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"\\n🔧 Treinando {nome}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(modelo, X_train_split, y_train_split, \n",
    "                               cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Treinar modelo completo\n",
    "    modelo.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Predições\n",
    "    y_pred = modelo.predict(X_val)\n",
    "    y_pred_proba = modelo.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Métricas\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    resultados[nome] = {\n",
    "        'modelo': modelo,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'auc_val': auc\n",
    "    }\n",
    "    \n",
    "    print(f\"   CV AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    print(f\"   Val AUC: {auc:.3f}\")\n",
    "\n",
    "# 3. Escolher melhor modelo\n",
    "melhor_modelo = max(resultados.keys(), key=lambda x: resultados[x]['auc_val'])\n",
    "print(f\"\\n🏆 Melhor modelo: {melhor_modelo}\")\n",
    "print(f\"   AUC: {resultados[melhor_modelo]['auc_val']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dca27c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Divisão dos dados:\n",
      "   Treino: (516, 32)\n",
      "   Validação: (130, 32)\n",
      "\n",
      "🔧 Treinando Random Forest...\n",
      "   CV AUC: 0.807 ± 0.056\n",
      "   Val AUC: 0.803\n",
      "   Acurácia: 0.777\n",
      "   Precisão: 0.784\n",
      "   Recall: 0.905\n",
      "   F1-Score: 0.840\n",
      "\n",
      "🔧 Treinando Gradient Boosting...\n",
      "   CV AUC: 0.776 ± 0.060\n",
      "   Val AUC: 0.824\n",
      "   Acurácia: 0.785\n",
      "   Precisão: 0.798\n",
      "   Recall: 0.893\n",
      "   F1-Score: 0.843\n",
      "\n",
      "🔧 Treinando Logistic Regression...\n",
      "   CV AUC: 0.763 ± 0.061\n",
      "   Val AUC: 0.770\n",
      "   Acurácia: 0.708\n",
      "   Precisão: 0.761\n",
      "   Recall: 0.798\n",
      "   F1-Score: 0.779\n",
      "\n",
      "================================================================================\n",
      "📊 TABELA COMPARATIVA DE MÉTRICAS\n",
      "================================================================================\n",
      "             Modelo        CV AUC Val AUC Acurácia Precisão Recall F1-Score\n",
      "      Random Forest 0.807 ± 0.056   0.803    0.777    0.784  0.905    0.840\n",
      "  Gradient Boosting 0.776 ± 0.060   0.824    0.785    0.798  0.893    0.843\n",
      "Logistic Regression 0.763 ± 0.061   0.770    0.708    0.761  0.798    0.779\n",
      "\n",
      "🏆 MELHOR MODELO: Gradient Boosting\n",
      "   AUC: 0.824\n",
      "   Acurácia: 0.785\n",
      "   F1-Score: 0.843\n",
      "\n",
      "📋 RELATÓRIO DETALHADO - Gradient Boosting\n",
      "==================================================\n",
      "\n",
      "📈 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Fracasso       0.75      0.59      0.66        46\n",
      "     Sucesso       0.80      0.89      0.84        84\n",
      "\n",
      "    accuracy                           0.78       130\n",
      "   macro avg       0.77      0.74      0.75       130\n",
      "weighted avg       0.78      0.78      0.78       130\n",
      "\n",
      "\n",
      "🔍 Matriz de Confusão:\n",
      "                  Predito\n",
      "              Fracasso  Sucesso\n",
      "Real Fracasso      27       19\n",
      "     Sucesso        9       75\n",
      "\n",
      "📊 Interpretação:\n",
      "   ✅ Verdadeiros Negativos (Fracasso previsto corretamente): 27\n",
      "   ❌ Falsos Positivos (Fracasso previsto como Sucesso): 19\n",
      "   ❌ Falsos Negativos (Sucesso previsto como Fracasso): 9\n",
      "   ✅ Verdadeiros Positivos (Sucesso previsto corretamente): 75\n",
      "\n",
      "🎯 TOP 10 FEATURES MAIS IMPORTANTES (Gradient Boosting):\n",
      "============================================================\n",
      " 1. relationships            : 0.2312\n",
      " 2. funding_total_usd        : 0.1581\n",
      " 3. age_last_milestone_year  : 0.1196\n",
      " 4. age_first_funding_year   : 0.1019\n",
      " 5. milestones               : 0.0823\n",
      " 6. id                       : 0.0665\n",
      " 7. avg_participants         : 0.0575\n",
      " 8. age_first_milestone_year : 0.0554\n",
      " 9. age_last_funding_year    : 0.0302\n",
      "10. category_code            : 0.0277\n"
     ]
    }
   ],
   "source": [
    "# MODELAGEM - MÚLTIPLOS ALGORITMOS COM MÉTRICAS COMPLETAS\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Dividir dados para validação\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"📊 Divisão dos dados:\")\n",
    "print(f\"   Treino: {X_train_split.shape}\")\n",
    "print(f\"   Validação: {X_val.shape}\")\n",
    "\n",
    "# 2. Testar múltiplos modelos\n",
    "modelos = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "metricas_comparacao = []\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"\\n🔧 Treinando {nome}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(modelo, X_train_split, y_train_split, \n",
    "                               cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Treinar modelo completo\n",
    "    modelo.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Predições\n",
    "    y_pred = modelo.predict(X_val)\n",
    "    y_pred_proba = modelo.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcular todas as métricas\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    resultados[nome] = {\n",
    "        'modelo': modelo,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'auc_val': auc,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    # Para tabela comparativa\n",
    "    metricas_comparacao.append({\n",
    "        'Modelo': nome,\n",
    "        'CV AUC': f\"{cv_scores.mean():.3f} ± {cv_scores.std():.3f}\",\n",
    "        'Val AUC': f\"{auc:.3f}\",\n",
    "        'Acurácia': f\"{accuracy:.3f}\",\n",
    "        'Precisão': f\"{precision:.3f}\",\n",
    "        'Recall': f\"{recall:.3f}\",\n",
    "        'F1-Score': f\"{f1:.3f}\"\n",
    "    })\n",
    "    \n",
    "    print(f\"   CV AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    print(f\"   Val AUC: {auc:.3f}\")\n",
    "    print(f\"   Acurácia: {accuracy:.3f}\")\n",
    "    print(f\"   Precisão: {precision:.3f}\")\n",
    "    print(f\"   Recall: {recall:.3f}\")\n",
    "    print(f\"   F1-Score: {f1:.3f}\")\n",
    "\n",
    "# 3. Tabela comparativa das métricas\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 TABELA COMPARATIVA DE MÉTRICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_metricas = pd.DataFrame(metricas_comparacao)\n",
    "print(df_metricas.to_string(index=False))\n",
    "\n",
    "# 4. Escolher melhor modelo (baseado em AUC)\n",
    "melhor_modelo_nome = max(resultados.keys(), key=lambda x: resultados[x]['auc_val'])\n",
    "melhor_resultado = resultados[melhor_modelo_nome]\n",
    "\n",
    "print(f\"\\n🏆 MELHOR MODELO: {melhor_modelo_nome}\")\n",
    "print(f\"   AUC: {melhor_resultado['auc_val']:.3f}\")\n",
    "print(f\"   Acurácia: {melhor_resultado['accuracy']:.3f}\")\n",
    "print(f\"   F1-Score: {melhor_resultado['f1_score']:.3f}\")\n",
    "\n",
    "# 5. Relatório detalhado do melhor modelo\n",
    "print(f\"\\n📋 RELATÓRIO DETALHADO - {melhor_modelo_nome}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "melhor_modelo_obj = melhor_resultado['modelo']\n",
    "y_pred_melhor = melhor_modelo_obj.predict(X_val)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n📈 Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_melhor, target_names=['Fracasso', 'Sucesso']))\n",
    "\n",
    "# Matriz de Confusão\n",
    "print(\"\\n🔍 Matriz de Confusão:\")\n",
    "cm = confusion_matrix(y_val, y_pred_melhor)\n",
    "print(f\"                  Predito\")\n",
    "print(f\"              Fracasso  Sucesso\")\n",
    "print(f\"Real Fracasso     {cm[0,0]:3d}      {cm[0,1]:3d}\")\n",
    "print(f\"     Sucesso      {cm[1,0]:3d}      {cm[1,1]:3d}\")\n",
    "\n",
    "# Interpretação da matriz\n",
    "vn, fp, fn, vp = cm.ravel()\n",
    "print(f\"\\n📊 Interpretação:\")\n",
    "print(f\"   ✅ Verdadeiros Negativos (Fracasso previsto corretamente): {vn}\")\n",
    "print(f\"   ❌ Falsos Positivos (Fracasso previsto como Sucesso): {fp}\")\n",
    "print(f\"   ❌ Falsos Negativos (Sucesso previsto como Fracasso): {fn}\")\n",
    "print(f\"   ✅ Verdadeiros Positivos (Sucesso previsto corretamente): {vp}\")\n",
    "\n",
    "# 6. Análise de importância das features (se for Random Forest ou Gradient Boosting)\n",
    "if melhor_modelo_nome in ['Random Forest', 'Gradient Boosting']:\n",
    "    print(f\"\\n🎯 TOP 10 FEATURES MAIS IMPORTANTES ({melhor_modelo_nome}):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    importances = melhor_modelo_obj.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Criar DataFrame com importâncias\n",
    "    df_importances = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importancia': importances\n",
    "    }).sort_values('Importancia', ascending=False)\n",
    "    \n",
    "    # Mostrar top 10\n",
    "    top_features = df_importances.head(10)\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['Feature']:25s}: {row['Importancia']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dcb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 GERANDO SUBMISSÃO COM GRADIENT BOOSTING\n",
      "=======================================================\n",
      "🏆 Modelo escolhido: Gradient Boosting Original\n",
      "📊 AUC de validação: 0.8230\n",
      "\n",
      "Treinando modelo final em todos os dados...\n",
      "   X_train: (646, 32)\n",
      "   y_train: (646,)\n",
      "   X_test: (277, 32)\n",
      "✅ Modelo treinado com sucesso!\n",
      "Gerando predições no test set...\n",
      "✅ Predições geradas: 277 valores binários (0 ou 1)\n",
      "✅ Usando coluna 'id' do dataset\n",
      "✅ Arquivo 'submission_gradient_boosting.csv' salvo!\n",
      "\n",
      "📋 INFORMAÇÕES DA SUBMISSÃO:\n",
      "   Linhas: 277\n",
      "   Colunas: ['id', 'labels']\n",
      "   Valores únicos: [0, 1]\n",
      "\n",
      "Primeiras 10 linhas:\n",
      "    id  labels\n",
      "0   70       1\n",
      "1   23       0\n",
      "2  389       1\n",
      "3  872       1\n",
      "4  920       0\n",
      "5  690       1\n",
      "6  588       0\n",
      "7  144       0\n",
      "8  875       1\n",
      "9  900       1\n",
      "\n",
      "📈 ESTATÍSTICAS DAS PREDIÇÕES (0/1):\n",
      "   Total de predições: 277\n",
      "   Fracasso (0): 88 (31.8%)\n",
      "   Sucesso (1): 189 (68.2%)\n",
      "\n",
      "📊 ESTATÍSTICAS DAS PROBABILIDADES (para análise):\n",
      "   Média das probabilidades: 0.6149\n",
      "   Mediana das probabilidades: 0.7283\n",
      "   Min: 0.0085\n",
      "   Max: 0.9856\n",
      "\n",
      "📊 DISTRIBUIÇÃO DAS PROBABILIDADES:\n",
      "   0.0-0.2:  46 startups ( 16.6%)\n",
      "   0.2-0.4:  27 startups (  9.7%)\n",
      "   0.4-0.6:  41 startups ( 14.8%)\n",
      "   0.6-0.8:  50 startups ( 18.1%)\n",
      "   0.8-1.0: 113 startups ( 40.8%)\n",
      "\n",
      "🔍 COMPARAÇÃO COM TREINO:\n",
      "   Taxa de sucesso no treino: 64.7%\n",
      "   Taxa de sucesso predita no teste: 68.2%\n",
      "   Diferença: +3.5%\n",
      "✅ Distribuições similares - predições consistentes!\n",
      "\n",
      "🎯 ANÁLISE DO THRESHOLD:\n",
      "   Threshold padrão usado: 0.5\n",
      "   Probabilidades > 0.5: 189\n",
      "   Predições como sucesso (1): 189\n",
      "   ✅ Consistência: OK\n",
      "\n",
      "🎉 SUBMISSÃO PRONTA!\n",
      "📁 Arquivo: submission_gradient_boosting.csv\n",
      "🏆 Modelo: Gradient Boosting Original\n",
      "📊 AUC esperado: 0.8230\n",
      "🎯 Formato: Valores binários (0=Fracasso, 1=Sucesso)\n",
      "💡 Pronto para upload no Kaggle!\n"
     ]
    }
   ],
   "source": [
    "# SUBMISSÃO FINAL - GRADIENT BOOSTING ORIGINAL\n",
    "print(\"📝 GERANDO SUBMISSÃO COM GRADIENT BOOSTING\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Usar o Gradient Boosting original (melhor modelo básico)\n",
    "modelo_final = resultados['Gradient Boosting']['modelo']\n",
    "auc_final = resultados['Gradient Boosting']['auc_val']\n",
    "\n",
    "print(f\"🏆 Modelo escolhido: Gradient Boosting Original\")\n",
    "print(f\"📊 AUC de validação: {auc_final:.4f}\")\n",
    "\n",
    "# Treinar modelo final em TODOS os dados de treino\n",
    "print(f\"\\nTreinando modelo final em todos os dados...\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "\n",
    "modelo_final.fit(X_train, y_train)\n",
    "print(\"✅ Modelo treinado com sucesso!\")\n",
    "\n",
    "# Fazer predições no test set\n",
    "print(\"Gerando predições no test set...\")\n",
    "# Predições binárias (0 ou 1) - não probabilidades\n",
    "predicoes_finais = modelo_final.predict(X_test)\n",
    "\n",
    "# Também calcular probabilidades para análise\n",
    "probabilidades = modelo_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"✅ Predições geradas: {len(predicoes_finais)} valores binários (0 ou 1)\")\n",
    "\n",
    "# Verificar coluna ID\n",
    "if 'id' in test.columns:\n",
    "    id_col = test['id']\n",
    "    print(\"✅ Usando coluna 'id' do dataset\")\n",
    "else:\n",
    "    id_col = range(len(test))\n",
    "    print(\"⚠️ Criando IDs sequenciais\")\n",
    "\n",
    "# Criar arquivo de submissão com valores binários\n",
    "submission = pd.DataFrame({\n",
    "    'id': id_col,\n",
    "    'labels': predicoes_finais  # 0 ou 1\n",
    "})\n",
    "\n",
    "# Salvar submission\n",
    "nome_arquivo = 'submission_gradient_boosting.csv'\n",
    "submission.to_csv(nome_arquivo, index=False)\n",
    "print(f\"✅ Arquivo '{nome_arquivo}' salvo!\")\n",
    "\n",
    "# Mostrar informações da submissão\n",
    "print(f\"\\n📋 INFORMAÇÕES DA SUBMISSÃO:\")\n",
    "print(f\"   Linhas: {len(submission)}\")\n",
    "print(f\"   Colunas: {list(submission.columns)}\")\n",
    "print(f\"   Valores únicos: {sorted(submission['labels'].unique())}\")\n",
    "print(\"\\nPrimeiras 10 linhas:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Estatísticas das predições BINÁRIAS\n",
    "print(f\"\\n📈 ESTATÍSTICAS DAS PREDIÇÕES (0/1):\")\n",
    "successo_predito = (predicoes_finais == 1).sum()\n",
    "fracasso_predito = (predicoes_finais == 0).sum()\n",
    "total = len(predicoes_finais)\n",
    "\n",
    "print(f\"   Total de predições: {total}\")\n",
    "print(f\"   Fracasso (0): {fracasso_predito} ({fracasso_predito/total*100:.1f}%)\")\n",
    "print(f\"   Sucesso (1): {successo_predito} ({successo_predito/total*100:.1f}%)\")\n",
    "\n",
    "# Estatísticas das probabilidades (para análise)\n",
    "print(f\"\\n📊 ESTATÍSTICAS DAS PROBABILIDADES (para análise):\")\n",
    "print(f\"   Média das probabilidades: {probabilidades.mean():.4f}\")\n",
    "print(f\"   Mediana das probabilidades: {np.median(probabilidades):.4f}\")\n",
    "print(f\"   Min: {probabilidades.min():.4f}\")\n",
    "print(f\"   Max: {probabilidades.max():.4f}\")\n",
    "\n",
    "# Comparar com distribuição do treino\n",
    "train_success_rate = y_train.mean()\n",
    "test_success_rate = successo_predito / total\n",
    "\n",
    "print(f\"\\n🔍 COMPARAÇÃO COM TREINO:\")\n",
    "print(f\"   Taxa de sucesso no treino: {train_success_rate:.1%}\")\n",
    "print(f\"   Taxa de sucesso predita no teste: {test_success_rate:.1%}\")\n",
    "print(f\"   Diferença: {test_success_rate - train_success_rate:+.1%}\")\n",
    "\n",
    "if abs(test_success_rate - train_success_rate) < 0.1:\n",
    "    print(\"✅ Distribuições similares - predições consistentes!\")\n",
    "elif test_success_rate > train_success_rate + 0.1:\n",
    "    print(\"📈 Modelo prevê mais sucessos que o observado no treino\")\n",
    "else:\n",
    "    print(\"📉 Modelo prevê menos sucessos que o observado no treino\")\n",
    "\n",
    "# Verificar threshold usado pelo modelo\n",
    "threshold_usado = 0.5\n",
    "acima_threshold = (probabilidades > threshold_usado).sum()\n",
    "print(f\"\\n🎯 ANÁLISE DO THRESHOLD:\")\n",
    "print(f\"   Threshold padrão usado: {threshold_usado}\")\n",
    "print(f\"   Probabilidades > {threshold_usado}: {acima_threshold}\")\n",
    "print(f\"   Predições como sucesso (1): {successo_predito}\")\n",
    "print(f\"   ✅ Consistência: {'OK' if acima_threshold == successo_predito else 'ERRO'}\")\n",
    "\n",
    "print(f\"\\n🎉 SUBMISSÃO PRONTA!\")\n",
    "print(f\"📁 Arquivo: {nome_arquivo}\")\n",
    "print(f\"🏆 Modelo: Gradient Boosting Original\")\n",
    "print(f\"📊 AUC esperado: {auc_final:.4f}\")\n",
    "print(f\"🎯 Formato: Valores binários (0=Fracasso, 1=Sucesso)\")\n",
    "print(f\"💡 Pronto para upload no Kaggle!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
